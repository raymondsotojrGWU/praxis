{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca811431-e342-4b34-b113-18af16113d26",
   "metadata": {},
   "source": [
    "#### Copyright Raymond Soto Jr. D.Eng(c).\n",
    "#### From Edge to Enterprise: \n",
    "#### Federated Learning Threat Classification with Heterogeneous Devices in Converged Energy Sector Networks\n",
    "#### Revised July 9th, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b746c-de26-46e3-a823-0dd5985298a2",
   "metadata": {},
   "source": [
    "# Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "id": "05bb1d24-1691-4596-8373-f15df4581d51",
   "metadata": {},
   "source": [
    "# Load Program Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab8a57d0-71f8-45d4-b81e-d61e07d88e2d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "98110321-5b1d-4a46-8006-f72001e7bc18",
   "metadata": {},
   "source": [
    "# imports the feature descriptions csv to work with the full traffic dataset\n",
    "unswnb15_features = pd.read_csv('UNSWB15_CSV_Files/UNSW-NB15_features.csv')\n",
    "# convert the feature names into a list\n",
    "feature_header = unswnb15_features['Name'].tolist()\n",
    "# import UNSW NB traffic dataset part 1 without headers, then map the list of headers, and silence low memory alert\n",
    "df1 = pd.read_csv('UNSWB15_CSV_Files/UNSW-NB15_1.csv',header=None, names=feature_header , low_memory=False)\n",
    "df2 = pd.read_csv('UNSWB15_CSV_Files/UNSW-NB15_2.csv',header=None, names=feature_header , low_memory=False)\n",
    "df3 = pd.read_csv('UNSWB15_CSV_Files/UNSW-NB15_3.csv',header=None, names=feature_header , low_memory=False)\n",
    "df4 = pd.read_csv('UNSWB15_CSV_Files/UNSW-NB15_4.csv',header=None, names=feature_header , low_memory=False)\n",
    "# print shape of each dataframe\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)\n",
    "# Assume 'label' column indicates 1 for malicious, 0 for normal\n",
    "# Ensure class distribution is examined, Print out the percentage of malicious vs normal instances\n",
    "label_col = 'label'\n",
    "print(df1[label_col].value_counts(normalize=True) * 100)\n",
    "print(df2[label_col].value_counts(normalize=True) * 100)\n",
    "print(df3[label_col].value_counts(normalize=True) * 100)\n",
    "print(df4[label_col].value_counts(normalize=True) * 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a9f2e1b-d132-4c83-8897-f9bf0c24fbe2",
   "metadata": {},
   "source": [
    "#### Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e8c8020f-bcb9-4691-a11e-56d8f306a269",
   "metadata": {},
   "source": [
    "# combined dataframe df1, df2, df3, d4\n",
    "df_combined = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "print(\"Combined DataFrame shape:\", df_combined.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "841d55d0-6c48-4e1e-9334-41aad5b7f894",
   "metadata": {},
   "source": [
    "# Feature Egnineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5ed2f-6796-42d9-9de2-1688b8277abd",
   "metadata": {},
   "source": [
    "#### Map Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5fc20d-d699-4ca1-a71a-66423c55f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map_protocol function to group network traffic protocols\n",
    "def map_protocol(proto):\n",
    "    if proto in [\"tcp\", \"udp\", \"sctp\", \"udt\", \"mux\", \"iso-tp4\", \"tp++\", \"ddp\", \"xtp\", \"vmtp\", \"mtp\", \"crudp\"]:\n",
    "        return \"transport\"\n",
    "    elif proto in [\"icmp\", \"igmp\", \"rsvp\", \"ptp\"]:\n",
    "        return 'control'\n",
    "    elif proto in [\"ospf\", \"egp\", \"igp\", \"idrp\", \"ipv6-route\", \"gre\", \"nsfnet-igp\", \"eigrp\", \"isis\", \"vrrp\"]:\n",
    "        return \"Routing\"\n",
    "    elif proto in [\"ip\", \"ipv6\", \"ipv6-frag\", \"ipv6-opts\", \"iso-ip\", \"ipnip\", \"ggp\", \"ipip\", \"ipx-n-ip\"]:\n",
    "        return \"Internet\"\n",
    "    elif proto in [\"pim\", \"rtp\", \"gmtp\", \"micp\", \"pgm\"]:\n",
    "        return \"Multicast\"\n",
    "    elif proto in [\"etherip\", \"l2tp\", \"encap\"]:\n",
    "        return \"Tunneling\"\n",
    "    elif proto in [\"arp\", \"stp\", \"ax.25\", \"fc\", \"ib\"]:\n",
    "        return \"Link\"\n",
    "    elif proto in [\"esp\", \"ipcomp\", \"secure-vmtp\"]:\n",
    "        return \"Security\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# Use map_protocol function to build a new column with categories from the Train set\n",
    "df_combined['protocol_category'] = df_combined['proto'].apply(map_protocol)\n",
    "# drop original proto column before OneHot encoding 'protocol_category'\n",
    "df_combined.drop(['proto'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6875b-f823-4a49-8bc3-77852d34da53",
   "metadata": {},
   "source": [
    "#### Map Coonnection State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aef90ce-4701-4806-bc80-2455ba782c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates map_connection_state function that maps connection state values to broader groups.\n",
    "def map_connection_state(state):\n",
    "    # Group states that indicate an established connection\n",
    "    if state in [\"CON\", \"ACC\", \"REQ\"]:\n",
    "        return \"established\"\n",
    "    # Group states that indicate termination of the connection\n",
    "    elif state in [\"FIN\", \"RST\", \"CLO\"]:\n",
    "        return \"terminated\"\n",
    "    # Group states that might indicate the handshake phase (if applicable)\n",
    "    elif state in [\"SYN\", \"SYN-ACK\"]:\n",
    "        return \"handshaking\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "# use map_connection_state function to build a new column with categories from the combined set\n",
    "df_combined['state_category'] = df_combined['state'].apply(map_connection_state)\n",
    "# drop original proto column before OneHot encoding 'protocol_category'\n",
    "df_combined.drop(['state'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab290d-3ac3-4a29-b77f-263155b125f1",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2cf77-f658-47f7-83d0-59ea5c599b7d",
   "metadata": {},
   "source": [
    "#### Feature Reduction: Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "41bd5521-fcd2-4788-b7cb-b5e62fc73570",
   "metadata": {},
   "source": [
    "# drop columns to align the shape of the dataframes\n",
    "df_combined.drop(['srcip','sport','dstip','service','stime','ltime','is_sm_ips_ports',\n",
    "                  'ct_ftp_cmd','ct_flw_http_mthd','is_ftp_login','attack_cat'],axis=1, inplace=True)\n",
    "\n",
    "# print dataframe shape\n",
    "print(f'DF Shape Combined Set {df_combined.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "33ddf625-783d-4a85-a71e-4bbf2362c672",
   "metadata": {},
   "source": [
    "#### Drops Rows of Invalid Hex and Regex and Convert to int"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7423c7a-2892-4ff4-9c65-a40ab4e5887a",
   "metadata": {},
   "source": [
    "# Build a boolean mask for rows where dsport looks like a hex string or regex\n",
    "mask_hex = df_combined['dsport'].astype(str).str.lower().str.startswith('0x')\n",
    "mask_digits = df_combined['dsport'].astype(str).str.fullmatch(r'\\d+')\n",
    "\n",
    "# Drop rows\n",
    "df_combined = df_combined.loc[~mask_hex].copy()\n",
    "df_combined = df_combined.loc[mask_digits].copy()\n",
    "\n",
    "# Convert the remaining dsport values to int\n",
    "df_combined['dsport'] = df_combined['dsport'].astype(int)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Remaining dsport dtype:\", df_combined['dsport'].dtype)\n",
    "print(\"Any hex rows left?\", df_combined['dsport'].astype(str).str.lower().str.startswith('0x').any())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7253740c-2ce0-4301-8fc2-d59e705bfadc",
   "metadata": {},
   "source": [
    "#### Convert to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b1bb25-6ff1-415f-ab7c-e5021776474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver label to a boolean type\n",
    "df_combined['label'] = df_combined['label'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d358ef-6cbd-4b6d-a3d3-20def1aa0d8b",
   "metadata": {},
   "source": [
    "#### Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "id": "b755313b-f274-4368-ac22-e47efc108d99",
   "metadata": {},
   "source": [
    "# OneHotEncoding\n",
    "# Select the columns to be one-hot encoded\n",
    "cols_to_encode = ['protocol_category','state_category']\n",
    "# Create the OneHotEncoder instance with the desired parameters\n",
    "encoder = OneHotEncoder(dtype=bool, sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the selected columns from the Combined set. This returns a NumPy array.\n",
    "encoded_array_combined = encoder.fit_transform(df_combined[cols_to_encode])\n",
    "# Retrieve the names for the new columns\n",
    "encoded_columns_combined = encoder.get_feature_names_out(cols_to_encode)\n",
    "# Convert the encoded array to a DataFrame. Preserve the index to merge correctly.\n",
    "encoded_df_combined = pd.DataFrame(encoded_array_combined, columns=encoded_columns_combined, index=df_combined.index)\n",
    "# Drop the original columns and concatenate the one-hot encoded DataFrame\n",
    "df_encoded_combined = pd.concat([df_combined.drop(columns=cols_to_encode), encoded_df_combined], axis=1)\n",
    "\n",
    "# Print to confirm\n",
    "print(f'DF Combined Shape Combined Set {df_encoded_combined.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0aa0e297-cc74-4de8-ae00-9464d04f93c1",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "70586f8c-47fa-4b24-9d19-b02bedc85f6c",
   "metadata": {},
   "source": [
    "# Split dataset to seperate the testing set before training and validation\n",
    "strat_col = 'label'\n",
    "\n",
    "df_encoded_combined, df_encoded_4 = train_test_split(\n",
    "    df_encoded_combined,\n",
    "    test_size=0.20,\n",
    "    stratify=df_encoded_combined[strat_col]\n",
    ")\n",
    "\n",
    "# Confirm\n",
    "print(f\"Train shape: {df_encoded_combined.shape},  Test shape: {df_encoded_4.shape}\")\n",
    "print(\"Train True proportion:\", df_encoded_combined[strat_col].mean())\n",
    "print(\"Test  True proportion:\", df_encoded_4[strat_col].mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1331f12-46ef-44c6-b72c-592492ab6f5b",
   "metadata": {},
   "source": [
    "# Training, Validation, & Testing Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3aafa0-d5a9-44fa-876d-bf6b05f190e2",
   "metadata": {},
   "source": [
    "#### Split Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "3105dc92-78c6-48a6-a701-f1991a61a057",
   "metadata": {},
   "source": [
    "# split the combined dataset into testing and validation\n",
    "X_train0 = df_encoded_combined.drop(['label'], axis=1) # set X as all features/predicator variables\n",
    "y_train0 = df_encoded_combined['label'].astype(bool) # set y as target variable\n",
    "\n",
    "# confirm distribution of malicous to normal: REMOVED for testing , random_state=42\n",
    "print(X_train0.shape)\n",
    "#print(X_val0.shape)\n",
    "\n",
    "print(y_train0.value_counts(normalize=True) * 100)\n",
    "#print(y_val0.value_counts(normalize=True) * 100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c10e8d0-9ed1-497a-bb38-46269c32dc1c",
   "metadata": {},
   "source": [
    "#### Split Testing"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c18ae47-df8c-4ffb-bff3-bb68a8712d06",
   "metadata": {},
   "source": [
    "# split the testing dataset\n",
    "X2 = df_encoded_4.drop(['label'], axis=1) # set X as all features/predicator variables\n",
    "y2 = df_encoded_4['label'].astype(bool) # set y as target variable\n",
    "# confirm distribution of malicous to normal\n",
    "print(y2.value_counts(normalize=True) * 100)\n",
    "print(X2.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd6de244-fdd0-4fed-a2d0-4e4ec8e533f9",
   "metadata": {},
   "source": [
    "#### Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cefb61-0424-4e42-a827-90c9ce48ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns in your training set\n",
    "numeric_cols = X_train0.select_dtypes(include=[np.number]).columns\n",
    "# Create and fit the scaler on TRAINING data\n",
    "scaler = StandardScaler()\n",
    "X_train0[numeric_cols] = scaler.fit_transform(X_train0[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0484f0aa-bfb4-40b1-9ccc-cbfc28196548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform your testing data \n",
    "X2[numeric_cols] = scaler.transform(X2[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3468c-b8e1-4d0c-bd86-c02062b76500",
   "metadata": {},
   "source": [
    "#### Confirm Standardization"
   ]
  },
  {
   "cell_type": "code",
   "id": "bbf35384-9732-43cf-9163-5911cdef0a0e",
   "metadata": {},
   "source": [
    "# Compute separately\n",
    "means0 = X_train0[numeric_cols].mean()\n",
    "stds0  = X_train0[numeric_cols].std()\n",
    "# Show the top 5 means and stds\n",
    "print(\"Means:\\n\", means0.head(), \"\\n\")\n",
    "print(\"Stds:\\n\",  stds0.head(), \"\\n\")\n",
    "\n",
    "means2 = X2[numeric_cols].mean()\n",
    "stds2  = X2[numeric_cols].std()\n",
    "# Show the top 5 means and stds\n",
    "print(\"Means:\\n\", means2.head(), \"\\n\")\n",
    "print(\"Stds:\\n\",  stds2.head(), \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77eb17ba-a6ab-4e5d-80a9-24c19328caa3",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "id": "919f905c-510f-48eb-83a7-748004d33a7d",
   "metadata": {},
   "source": [
    "# SMOTE will increase the count of miniroty items\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train0, y_train0)\n",
    "print(\"Before SMOTE:\", y_train0.value_counts())\n",
    "print(\"After SMOTE:\", y_train_smote.value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e16064c5-a9cf-4967-8e2b-3431289894a0",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bdaaf-3e74-4884-9b0e-865eaa36c442",
   "metadata": {},
   "source": [
    "#### Convert to Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c12b310-5360-4811-b8f7-05133e0410d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow operations and the tf.data.Dataset API expect data \n",
    "# in a format that it can work with efficientlyâ€”typically NumPy arrays or tensors.\n",
    "X_train_smote_np = X_train_smote.to_numpy(dtype=np.float32)\n",
    "y_train_smote_np = y_train_smote.to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed3c03-72ad-44ad-a6cb-f89d6fd4c33f",
   "metadata": {},
   "source": [
    "### StratifiedKFold, Early Stop: Val_Recall, Model Checkpoint, Best Model Saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251de7d-51ea-4af1-ab45-0eec823b206a",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (MLP) Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1097ffb-43c7-454b-9a46-2e6a406d2c15",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# --- Define your model-creation function ---\n",
    "def create_mlp_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(47,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\n",
    "                      'binary_accuracy',\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Recall(name='recall')\n",
    "                  ])\n",
    "    return model\n",
    "\n",
    "# --- Define Callbacks ---\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_recall',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Cross Validation Setup ---\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_no = 1\n",
    "fold_histories = []\n",
    "best_model_overall = None\n",
    "best_val_recall = 0.0\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_smote_np, y_train_smote_np):\n",
    "    print(f'--- Fold {fold_no} ---')\n",
    "    \n",
    "    # Split data into training and validation for this fold\n",
    "    X_train_fold, X_val_fold = X_train_smote_np[train_index], X_train_smote_np[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_smote_np[train_index], y_train_smote_np[val_index]\n",
    "    \n",
    "    # Create tf.data.Dataset objects for training and validation\n",
    "    train_dataset_fold = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))\n",
    "    train_dataset_fold = train_dataset_fold.shuffle(buffer_size=len(X_train_fold)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset_fold = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold))\n",
    "    val_dataset_fold = val_dataset_fold.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create a new instance of your model for each fold\n",
    "    model = create_mlp_model()\n",
    "    \n",
    "    # Setup a ModelCheckpoint callback for this fold to save the best model based on val_loss\n",
    "    checkpoint_filepath = f'best_MLP_model_fold_{fold_no}.h5'\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_recall',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset_fold,\n",
    "        epochs=100,\n",
    "        validation_data=val_dataset_fold,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    fold_histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model on the validation fold\n",
    "    scores = model.evaluate(val_dataset_fold, verbose=0)\n",
    "    print(f\"Fold {fold_no} - Loss: {scores[0]:.4f} - Accuracy: {scores[1]:.4f}\")\n",
    "    \n",
    "    # Save the best overall model based on the final validation loss of the fold\n",
    " \n",
    "    current_best_recall = max(history.history['val_recall'])\n",
    "    \n",
    "    if current_best_recall > best_val_recall:\n",
    "        best_val_recall   = current_best_recall\n",
    "        best_model_overall = model\n",
    "        \n",
    "    fold_no += 1\n",
    "\n",
    "# --- Save the Best Overall Model ---\n",
    "if best_model_overall:\n",
    "    best_model_overall.save('best_MLP_model_overall.h5')\n",
    "    print('Best overall model saved as best_MLP_model_overall.keras')\n",
    "\n",
    "# --- Timing ---\n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Current date and time:\", current_datetime)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total elapsed time: {:.2f} seconds\".format(elapsed_time))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72b465ee-de0d-45cf-b86a-16bf79d5b1dd",
   "metadata": {},
   "source": [
    "#### MLP Plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "b764163d-7a5f-44d5-bffd-350a7e9ec7fe",
   "metadata": {},
   "source": [
    "# --- Plotting Metrics for Each Fold ---\n",
    "for i, history in enumerate(fold_histories, 1):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'MLP Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Binary Accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history['binary_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_binary_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'MLP Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history['precision'], label='Train Precision')\n",
    "    plt.plot(epochs, history['val_precision'], label='Val Precision')\n",
    "    plt.title(f'MLP Fold {i} Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Recall\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history['recall'], label='Train Recall')\n",
    "    plt.plot(epochs, history['val_recall'], label='Val Recall')\n",
    "    plt.title(f'MLP Fold {i} Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "874452d7-fba8-4eff-8e0c-6453a804b9ce",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network (CNN) Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9fcfc40-9987-4bce-8dd8-9a9606c76ab1",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# --- Define your model-creation function using CNN layers ---\n",
    "def create_cnn_model():\n",
    "    model = models.Sequential([\n",
    "        # Input shape is (47,) and we reshape to (47, 1) for the Conv1D layers.\n",
    "        layers.Input(shape=(47,)),\n",
    "        layers.Reshape((47, 1)),\n",
    "        # First convolutional block: 128 filters, kernel size 3, RELU activation.\n",
    "        layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # Second convolutional block: 64 filters.\n",
    "        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        # Flatten the output and pass through dense layers.\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\n",
    "                      'binary_accuracy',\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Recall(name='recall')\n",
    "                  ])\n",
    "    return model\n",
    "\n",
    "# --- Define Callbacks ---\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_recall',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Cross Validation Setup ---\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_no = 1\n",
    "fold_histories = []\n",
    "best_model_overall = None\n",
    "best_val_recall = 0.0\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_smote_np, y_train_smote_np):\n",
    "    print(f'--- Fold {fold_no} ---')\n",
    "    \n",
    "    # Split data into training and validation for this fold\n",
    "    X_train_fold, X_val_fold = X_train_smote_np[train_index], X_train_smote_np[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_smote_np[train_index], y_train_smote_np[val_index]\n",
    "    \n",
    "    # Create tf.data.Dataset objects for training and validation\n",
    "    train_dataset_fold = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))\n",
    "    train_dataset_fold = train_dataset_fold.shuffle(buffer_size=len(X_train_fold)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset_fold = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold))\n",
    "    val_dataset_fold = val_dataset_fold.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create a new instance of your CNN-based model for each fold\n",
    "    model = create_cnn_model()\n",
    "    \n",
    "    # Setup a ModelCheckpoint callback for this fold to save the best model based on val_loss\n",
    "    checkpoint_filepath = f'best_CNN_model_fold_{fold_no}.h5'\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_recall',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset_fold,\n",
    "        epochs=100,\n",
    "        validation_data=val_dataset_fold,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    fold_histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model on the validation fold\n",
    "    scores = model.evaluate(val_dataset_fold, verbose=0)\n",
    "    print(f\"Fold {fold_no} - Loss: {scores[0]:.4f} - Accuracy: {scores[1]:.4f}\")\n",
    "    \n",
    "    # Save the best overall model based on the final validation loss of the fold\n",
    "    current_best_recall = max(history.history['val_recall'])\n",
    "    \n",
    "    if current_best_recall > best_val_recall:\n",
    "        best_val_recall   = current_best_recall\n",
    "        best_model_overall = model\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "# --- Save the Best Overall Model ---\n",
    "if best_model_overall:\n",
    "    best_model_overall.save('best_CNN_model_overall.h5')\n",
    "    print(\"Best overall model saved as best_CNN_model_overall.h5\")\n",
    "\n",
    "# --- Timing ---\n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Current date and time:\", current_datetime)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total elapsed time: {:.2f} seconds\".format(elapsed_time))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23ded761-5bfe-448b-9e52-3e4da7419dfa",
   "metadata": {},
   "source": [
    "#### CNN Plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "d810c72c-acf0-4160-892c-b8a758db4620",
   "metadata": {},
   "source": [
    "# --- Plotting Metrics for Each Fold ---\n",
    "for i, history in enumerate(fold_histories, 1):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'CNN Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Binary Accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history['binary_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_binary_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'CNN Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history['precision'], label='Train Precision')\n",
    "    plt.plot(epochs, history['val_precision'], label='Val Precision')\n",
    "    plt.title(f'CNN Fold {i} Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Recall\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history['recall'], label='Train Recall')\n",
    "    plt.plot(epochs, history['val_recall'], label='Val Recall')\n",
    "    plt.title(f'CNN Fold {i} Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b89df1fa-73cd-4ba6-bbb4-54f333e92c83",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network (RNN) Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "0768aa40-9ecb-4663-8f48-5aef3694ecc8",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# --- Define your model-creation function using RNN layers ---\n",
    "def create_rnn_model():\n",
    "    model = models.Sequential([\n",
    "        # Input shape is (47,); reshape to (47, 1) for the RNN layers.\n",
    "        layers.Input(shape=(47,)),\n",
    "        layers.Reshape((47, 1)),\n",
    "        # First SimpleRNN layer with 128 units returning sequences for stacking.\n",
    "        layers.SimpleRNN(128, activation='relu', return_sequences=True),\n",
    "        # Second SimpleRNN layer with 64 units; does not return sequences.\n",
    "        layers.SimpleRNN(64, activation='relu', return_sequences=False),\n",
    "        # Final Dense layer for binary classification.\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[\n",
    "                      'binary_accuracy',\n",
    "                      tf.keras.metrics.Precision(name='precision'),\n",
    "                      tf.keras.metrics.Recall(name='recall')\n",
    "                  ])\n",
    "    return model\n",
    "\n",
    "# --- Define Callbacks ---\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_recall',\n",
    "    patience=20,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- Cross Validation Setup ---\n",
    "num_folds = 5\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_no = 1\n",
    "fold_histories = []\n",
    "best_model_overall = None\n",
    "best_val_recall = 0.0\n",
    "\n",
    "for train_index, val_index in skf.split(X_train_smote_np, y_train_smote_np):\n",
    "    print(f'--- Fold {fold_no} ---')\n",
    "    \n",
    "    # Split data into training and validation for this fold\n",
    "    X_train_fold, X_val_fold = X_train_smote_np[train_index], X_train_smote_np[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_smote_np[train_index], y_train_smote_np[val_index]\n",
    "    \n",
    "    # Create tf.data.Dataset objects for training and validation\n",
    "    train_dataset_fold = tf.data.Dataset.from_tensor_slices((X_train_fold, y_train_fold))\n",
    "    train_dataset_fold = train_dataset_fold.shuffle(buffer_size=len(X_train_fold)).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset_fold = tf.data.Dataset.from_tensor_slices((X_val_fold, y_val_fold))\n",
    "    val_dataset_fold = val_dataset_fold.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Create a new instance of your RNN-based model for each fold\n",
    "    model = create_rnn_model()\n",
    "    \n",
    "    # Setup a ModelCheckpoint callback for this fold to save the best model based on val_loss.\n",
    "    # Using HDF5 format (.h5) to avoid unsupported \"options\" arguments in native Keras format.\n",
    "    checkpoint_filepath = f'best_RNN_model_fold_{fold_no}.h5'\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_recall',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset_fold,\n",
    "        epochs=100,\n",
    "        validation_data=val_dataset_fold,\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    fold_histories.append(history.history)\n",
    "    \n",
    "    # Evaluate the model on the validation fold\n",
    "    scores = model.evaluate(val_dataset_fold, verbose=0)\n",
    "    print(f\"Fold {fold_no} - Loss: {scores[0]:.4f} - Accuracy: {scores[1]:.4f}\")\n",
    "    \n",
    "    # Save the best overall model based on the final validation loss of the fold\n",
    "    current_best_recall = max(history.history['val_recall'])\n",
    "    \n",
    "    if current_best_recall > best_val_recall:\n",
    "        best_val_recall   = current_best_recall\n",
    "        best_model_overall = model\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "# --- Save the Best Overall Model ---\n",
    "if best_model_overall:\n",
    "    best_model_overall.save('best_RNN_model_overall.h5')\n",
    "    print(\"Best overall model saved as best_RNN_model_overall.h5\")\n",
    "\n",
    "# --- Timing ---\n",
    "current_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Current date and time:\", current_datetime)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Total elapsed time: {:.2f} seconds\".format(elapsed_time))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0efe4de7-d412-4c40-b56e-9a9239253ab2",
   "metadata": {},
   "source": [
    "#### RNN Plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "5cb58570-b591-4af9-ad6c-30fa1f69a32a",
   "metadata": {},
   "source": [
    "# --- Plotting Metrics for Each Fold ---\n",
    "for i, history in enumerate(fold_histories, 1):\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs, history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'RNN Fold {i} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Binary Accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs, history['binary_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_binary_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'RNN Fold {i} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Precision\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs, history['precision'], label='Train Precision')\n",
    "    plt.plot(epochs, history['val_precision'], label='Val Precision')\n",
    "    plt.title(f'RNN Fold {i} Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Recall\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs, history['recall'], label='Train Recall')\n",
    "    plt.plot(epochs, history['val_recall'], label='Val Recall')\n",
    "    plt.title(f'RNN Fold {i} Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5fbf5a39-2dad-48c4-9f39-1a5690b430d0",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee5757-30d7-49a1-a0df-62a061894591",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "id": "b788b25a-8653-477b-a229-07afde50db9a",
   "metadata": {},
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('best_MLP_model_overall.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Convert to NumPy arrays with appropriate data types\n",
    "X2_np = X2.to_numpy(dtype=np.float32)\n",
    "y2_np = y2.to_numpy(dtype=np.int32)\n",
    "\n",
    "# Use the model to predict probabilities, then convert to binary predictions using a threshold of 0.5\n",
    "y_pred_prob = model.predict(X2_np)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y2_np, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y2_np, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.title(\"Confusion Matrix for MLP Testing\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5d84302d-688a-4db3-998f-aee388ffb749",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "7acd9d6a-6faf-471a-8af6-dbd11de3f93a",
   "metadata": {},
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('best_CNN_model_overall.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Convert to NumPy arrays with appropriate data types\n",
    "X2_np = X2.to_numpy(dtype=np.float32)\n",
    "y2_np = y2.to_numpy(dtype=np.int32)\n",
    "\n",
    "# Use the model to predict probabilities, then convert to binary predictions using a threshold of 0.5\n",
    "y_pred_prob = model.predict(X2_np)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y2_np, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y2_np, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.title(\"Confusion Matrix for CNN Testing\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccc25ffc-e28d-4bea-bb85-8cde598fd842",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "id": "e475124e-6cb6-4b47-b16d-e3eac1a75f55",
   "metadata": {},
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('best_RNN_model_overall.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Convert to NumPy arrays with appropriate data types\n",
    "X2_np = X2.to_numpy(dtype=np.float32)\n",
    "y2_np = y2.to_numpy(dtype=np.int32)\n",
    "\n",
    "# Use the model to predict probabilities, then convert to binary predictions using a threshold of 0.5\n",
    "y_pred_prob = model.predict(X2_np)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y2_np, y_pred))\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y2_np, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.title(\"Confusion Matrix for RNN Testing\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13291f84-3586-4309-b480-ebde1e2a3c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
